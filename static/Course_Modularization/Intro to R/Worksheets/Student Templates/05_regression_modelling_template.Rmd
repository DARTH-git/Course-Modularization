---
title: "Intro to R for Decision Modeling"
author: "SickKids and DARTH"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
subtitle: Regression Modelling
---

This worksheet provides and introduction to regression modelling using `R`. This session is split into the following sections:

1. Simple linear regression modelling and diagnostics

2. Multiple linear regression modelling and diagnostics

3. Generalized linear regression modelling and diagnostics

Throughout the course, we will demonstrate code and leave some empty *code chunks* for you to fill in. We will also provide solutions after the session.

Feel free to modify this document with your own comments and clarifications. 

# 1.0 Load data into `R`
 
Before we begin this session, we need to load the Framingham dataset into `R` and load required packages.
 
```{r, warning=F, message=F}
data <- read.csv('framingham.csv', header = TRUE)
# install.packages(c('olsrr', 'pROC', 'ResourceSelection', 'nnet', 'survival', 'survminer'))
```
 
We will modify some categorical variables so they are encoded as categories rather than numbers. This helps us understand and visualize the results better.
 
```{r, warning = FALSE, message = FALSE}
library(dplyr)
data1 <- data %>% 
             mutate(SEX = ifelse(!is.na(SEX), 
                                  ifelse(SEX == 1, 
                                         'male', 'female'), 
                                          NA)) %>%
             mutate(PREVSTRK = ifelse(!is.na(PREVSTRK), 
                                       ifelse(PREVSTRK == 1, 
                                              'yes', 'no'), 
                                               NA)) %>%
             mutate(PREVMI = ifelse(!is.na(PREVMI), 
                                     ifelse(PREVMI == 1, 
                                            'yes', 'no'), 
                                             NA)) %>% 
             mutate(DIABETES = ifelse(!is.na(DIABETES), 
                                       ifelse(DIABETES== 1, 
                                              'yes', 'no'), 
                                             NA)) %>%
             mutate(CURSMOKE = ifelse(!is.na(CURSMOKE), 
                                       ifelse(CURSMOKE== 1, 
                                              'yes', 'no'), 
                                             NA)) %>%
             mutate(BPMEDS = ifelse(!is.na(BPMEDS), 
                                     ifelse(BPMEDS== 1, 
                                            'yes', 'no'), 
                                             NA))
```

# 1. Simple linear regression modelling and diagnostics

The `lm()` function in `R` allows us to construct simple and multiple linear regression models.

The general syntax is for a simple linear regression model is

`lm(outcome ~ predictor, data = your data)`

We will try to predict the `outcome` low density lipoprotein cholesterol (`LDLC`) with different predictors. 

**EXERCISE 1**  Construct a simple linear regression model with the *outcome* `LDLC` and the *predictor* `BMI`.

```{r}
# Your turn

```

We can use the `summary()` function to display information about the regression. To do this, it is useful to store your regression model as an *object* in `R` so we can reuse the regression fit for different purposes. We can store a regression model as an *object* with the following code:

`model_predictor <- lm(outcome ~ predictor, data = your data)`

**EXERCISE 2** Fit the `LDLC` against other predictors. Predictors that make clinical sense include age (`AGE`), prevalent myocardial infarction (`PREVMI`), prevalent stroke (`PREVSTRK`), current smoker (`CURSMOKE`). Use `summary()` to display the results.

```{r}
# Your turn

```

In this following section, we will use the model of `LDLC` against `BMI` to test the model assumptions:

```{r}
model_BMI <- lm(LDLC ~ BMI, data = data1)
summary(model_BMI)
```

The following are assumptions of a linear regression model;

- Linearity: The relationship between the outcome and the predictor(s) is linear.
- Homoscedasticity (equal variance): The variance of residuals are the same across the sample .
- Independence: Observations are independent of each other.
- Normality: The residuals of the model are normally distributed.

The plot of residuals versus fitted (predicted) values is useful for checking the assumptions of linearity and equal variance. 

- If the residuals are not too far away from 0, the linearlity assumption is met. 
- If the residuals do not have a pattern, the homoscedasticity assumption is met (You can also check this with the scale-location plot).

A normal Q-Q plot can be used to assess the normality assumption. A plot of "Cook's distance" can then be used to identify potential outliers. It measures the "influence" of each observation on regression coefficients.

The following code extracts and plots the residuals of our model:

```{r}
# create a dataset with non-missing LDLC and BMI
data2 <- data1[!is.na(data1$LDLC) & !is.na(data1$BMI),]
# create a dataset with observed, fitted and residual values
table <- data.frame(observed  = data2$LDLC, 
                    fitted    = model_BMI$fitted.values, 
                    residuals = data2$LDLC - model_BMI$fitted.values)
head(table) 

# plot the reesiduals
plot(model_BMI$residuals)
```

The following code produces the four types of plots mentioned above:

```{r}
# Asessing regression fit
par(mfrow = c(2,2)) # displays 2 graphs on the top and the bottom
plot(model_BMI, which = 1:4)
```

- Calling `plot()` on a regression model gives different diagnostic plots; you can investigate these plots and some alternatives by typing `?plot.lm` and accessing the help file for plotting linear models.

# 2. Multiple linear regression and diagnostics

Now we want to add more variables into the model, so we will fit a multiple linear regression model. The following command constructs a multiple linear regression model with LDLC as the outcome and age, BMI, heart rate, prevalent myocardial infarction (`PREVMI`), prevalent stroke status (`PREVSTRK`) and current smoking status (`CURSMOKE`) as the independent variables. 

```{r}
model2 <- lm(LDLC ~ AGE + BMI + HEARTRTE + PREVMI + PREVSTRK + CURSMOKE, data = data1)
``` 

All categories of a factor variable are compared against the reference category when interpreting results from a linear model.

Now, we will turn our categorical predictors into factors.

```{r}
# PREVMI
# change to a factor variable
data1$PREVMI <- factor(data1$PREVMI) 
# check the levels 
levels(data1$PREVMI) 

# CURSMOKE
# change to a factor variable
data1$CURSMOKE <- factor(data1$CURSMOKE) 
# check the levels
levels(data1$CURSMOKE) 
```

**EXERCISE 3** Make `PREVSTRK` a factor variable and check the reference category.

```{r}
# Your turn

```

**EXERCISE 4** Use the multiple linear regression model with our updated factor variables and display the model summary.

```{r}
# Your turn

```

**EXERCISE 5** Run regression diagnostics on the previous model and comment.

```{r}
# Your turn

```

**Variable selection**

Looking at the model summary above, age is not statistically significant. One way to test age individually is to have a model with age and a model with the same predictors except age and do a nested model comparison using ANOVA.

```{r}
model_no_age <- lm(LDLC ~ BMI + HEARTRTE + PREVMI + PREVSTRK
                   + CURSMOKE, data = data1)
anova(model_no_age, model2)
```

The ANOVA F-test shows that age is not statistically significant; it does not help predict the outcome on top of the other predictors.

Sometimes the models we wish to compare are not necessarily nested within each other. In such cases, we can use the folowing variable election methods:

- Stepwise forward regression
- Stepwise backward regression
- Stepwise regression

The package `olsrr` is very useful for conducting various variable selection algorithms.

Remember to install the `olsrr` package if you haven't already done so!

The following code runs stepwise forward regression on our model:

```{r, warning=F, message=F}
library(olsrr)
ols_step_forward_p(model2)
```

**EXERCISE 6** Run stepwise backward regression (`ols_step_backward_p()`) and stepwise regression (`ols_step_both_p()`) on our model.

```{r}
# Your turn
# Stepwise backgward regression

```

```{r}
# Your turn
# Stepwise regression

```

# 3. Generalized linear regression modelling and diagnostics

Linear models are only suitable for continuous response/outcome variables. If we have categorical outcomes, we have to use generalized linear regression models (GLM) --- an extension of linear regression models.

Logistic (logit) regression models are by far the most widely used and popular GLMs as they are suitable for many types of categorical outcomes. Logit models will be the focus of this section. 

Supppose we are interested in what predicting prevalent stroke status, we can fit a logistic regression model.

We want to make sure that our outcome is binary (0 and 1) first:

```{r}
data1$PREVSTRK <- as.character(data1$PREVSTRK)
data1$PREVSTRK[(is.na(data1$PREVSTRK) == FALSE) & (data1$PREVSTRK == 'yes')] <- 1 
data1$PREVSTRK[(is.na(data1$PREVSTRK) == FALSE) & (data1$PREVSTRK != 1)] <- 0
data1$PREVSTRK <- as.factor(data1$PREVSTRK)
```

We then fit a logit model with age being the only predictor:

```{r}
logit_model1 <- glm(PREVSTRK ~ AGE, family = "binomial", 
                    data = data1)
summary(logit_model1)
```

Different "families" of glm correspond to different types of outcome variable. The "binomial" family deals with categorical outcome variables.

We can assess our model fit as well the the predictive power of our model by using the following techniques:

- Hosmer-Lemeshow Test

- Receiver operating characteristic (ROC) curve and area under the curve (AUC)

The following code runs the Hosmer-Lemeshow test on our logit model using the `hoslem.test()` function from the `ResourceSelection` package.

```{r, warning = F, message = F}
library(ResourceSelection)
hoslem.test(data1$PREVSTRK, fitted(logit_model1), g = 10)
```

- The `hoslem.test()` function takes the 1st argument as the outcome variable, the 2nd argument as the fitted values from the logit model (can be extracted with the function `fitted()`) and the 3rd argument as the number of subgroups.

- If the test is statistically significant, the model is a poor fit of the data.

We are most concerned with the area under the ROC curve (AUC), the bigger the better. An AUC greater than 0.70 indicates that our model does a good job at discriminating between the two categories that comprise the outcome variable.

The following code uses functions from the `pROC` package to plot the ROC curve and calculate the AUC:

```{r, warning=F, message=F}
library(pROC)
# set, outcome variable followed by fitted values from model
roc1 <- roc(data1$PREVSTRK, logit_model1$fitted.values)  
# plot the ROC curve
plot(roc1, col = 'red', legacy.axes = TRUE)
# calculate the AUC
auc(roc1)                                              
```

Here we fit a logit model with `PREVSTRK` as the outcome variable and `AGE`, `BMI`, `CURSMOKE`, `HEARTRTE`, `SYSBP`, and `LDLC` as the predictors. 

```{r}
# Run and view model summary
logit_model3 <- glm(PREVSTRK ~ AGE + BMI + CURSMOKE + HEARTRTE 
                    + SYSBP + LDLC, 
                    family = "binomial", data = data1)
summary(logit_model3)
```

The `step()` function performs forward, backward and both stepwise regression for glms using the AIC:

Before running `step()`, we need to take out observations with missing values.

**EXERCISE 7** Create a dataset with only the variables in our model and take out any observation with missing values in any of the columns. 

```{r, warning=F, message=F}
# Your turn

```

Now that we have a dataset with only the variables we are going to put in our model with no missing values. We can fit the model again on this new dataset, and use the `step()` on it.

```{r}
logit_model3 <- glm(PREVSTRK ~ AGE + BMI + CURSMOKE + HEARTRTE + 
                      SYSBP + LDLC, 
                    family = "binomial", data = data2)
step(logit_model3, direction = "backward")
```

**EXERCISE 8** Perform stepwise forward and backward (both) regression on our model. Are the chosen predictors the same for the two methods?

```{r}
# Your turn

```

**EXERCISE 9** 

- Fit a logistic regression model on smoking status and use the same predictors as our previous model except smoking status.

- Display model summary and interpret the results.

- Run model diagnostic and comment on the goodness of fit.

- Run stepwise variable selection using the AIC.

```{r}
# Your turn
# Changing smoking status to a binary variable

# select only the variables we need and exclude any observation with missing values

# fit model

```

```{r}
# Your turn
# assess goodness of fit
# Hosmer-Lemeshow test

# ROC and AUC

                                      
```

```{r}
# Your turn
# stepwise variable selection 

```

**EXERCISE 10** 

- Fit a multinomial logistic regression model on systolic blood pressure (SYSBP) and use AGE and BMI as your predictors.
  
  1) Make SYSBP a categorical variable called SYSBP_CAT using the following categories:
     Normal    < 120
     Elevated  120-130
     High      >130
     
  2) Make SYSBP_CAT a factor and make "Normal" your reference category.
  
  3) Run your regression.
  
```{r}
# prep
library(nnet)

# Your turn
# Make SYSBP a categorical variable

```

```{r}
# Your turn
# Make SYSBP a factor with "Normal" as the reference category

```

```{r}
# Your turn
# Run your regresssion

```

**EXERCISE 11** 

- Make a Kaplan Meier plot using CVD and TIMECVD 
- Title your plot "CVD KM Plot"

```{r}
# prep
library(survival)
library(survminer)

# Your turn
# Make your Kaplan Meier plot 

```

**EXERCISE 12** 

- Fit a cox model using CVD and TIMECVD, adjusting for BMI
- Plot you cox model

```{r}
# Your turn
# Run your regresssion

```

```{r}
# Your turn
# Plot your model and view your plot

```
